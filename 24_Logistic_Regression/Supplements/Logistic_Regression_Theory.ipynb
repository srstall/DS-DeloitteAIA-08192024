{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Modeling the Log Likelihood\n",
    "\n",
    "In classification problems, the target variable in the model is categorical data, which cannot be fitted with a linear model directly. To design a better predictive model structure, we first define how the computer can understand the category in the target, then use a different approach to have the computer return a probability of some category given the value of the predictors.\n",
    "\n",
    "How can a computer understand a binary target?\n",
    "\n",
    "The most obvious way to get the computer to understand a binary output or target is to assign numerical values to the target: \n",
    "\n",
    "1 represents Yes, True, class 1, etc ... and   \n",
    "0 represents No, False, class 2, etc ...   \n",
    "\n",
    "This is what a computer can read and understand; usually we call it \"Boolean\".\n",
    "\n",
    "How can we ask the computer to calculate a probability of the outcome given some predictor's value?\n",
    "\n",
    "In ML, we usually use what we call an \"**Activation Function**\" to normalize the output value of any input values. One of the activation functions we used in Logistic Regression modeling is the **Sigmoid** function.\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Sometimes you may see the sigmoid function in the following form by multiplying the right hand side by $\\frac{e^z}{e^z}$,\n",
    "\n",
    "$$y = \\frac{e^z}{e^z+1}$$\n",
    "\n",
    "where y is the predicted value and z is an input to this sigmoid function, the inverse of which is called the \"**logit function**\". \n",
    "\n",
    "$$logit = \\beta_0 + \\beta_1(x_1) + ... \\beta_n(x_n)$$\n",
    "\n",
    "Why the Sigmoid function?\n",
    "\n",
    "The behavior of the sigmoid function always returns the S-shape curve where the predicted value always falls between 0 and 1. The predicted value therefore can be interpreted as the probability of the target given some predictor values.\n",
    "\n",
    "So far, everything seems to be making sense, but how do we model the sigmoid function or probability?  Before we model the probability, we need to take little steps to transform the sigmoid function,\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$1 + e^{-z} = \\frac{1}{y}$$\n",
    "\n",
    "$$e^{-z} = \\frac{1}{y} - 1$$\n",
    "\n",
    "$$ln(e^{-z}) = ln(\\frac{1}{y} - \\frac{y}{y})$$\n",
    "\n",
    "$$-z = ln(\\frac{1 - y}{y})$$\n",
    "\n",
    "$$z = -ln(\\frac{1 - y}{y})$$\n",
    "\n",
    "$$z = ln(\\frac{y}{1-y})$$\n",
    "\n",
    "The final equation is called the inverse sigmoid function, which demonstrates how we can use the z input to estimate the log of the odds ratio ($\\frac{y}{1-y}$), where y is the probability of success (yes, true, or class 1 in general), and (1 - y) is the probability of fail (no, false, or class 2 in general).\n",
    "\n",
    "Putting everything together, now we have an equation,\n",
    "\n",
    "$$ln(\\frac{y}{1-y}) = z$$\n",
    "\n",
    "$$ln(\\frac{y}{1-y}) = \\beta_0 + \\beta_1(x_1) + ... \\beta_n(x_n)$$ \n",
    "\n",
    "Since the input z is a linear combination of the predictors to predict the **log-odds ratio**, we can apply some of the properties we have learned about linear regression in the logistic regression model.\n",
    "\n",
    "More technically speaking, we are using the derivative of the logit function to **maximize the likelihood** with a given set of predictors. Here is a resource for reading the maximum likelihood process. \n",
    "\n",
    "Resource: https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf\n",
    "\n",
    "#### Prediction of the Model!!\n",
    "\n",
    "Keep in mind that when we run the logistic regression model, the returned predicted value is the log-odds ratio of the classification problem, so we need to always transform it back to the probability measure, $y$, for prediction. The transformation is relatively easy and SKlearn can handle the transformation for you,\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{1+e^{-\\hat{z}}}$$\n",
    "\n",
    "Also, the threshold for the classification decision is based on domain knowledge and classification metrics, for example\n",
    "\n",
    "$\\hat{y}$ > 50%, classify as category A, otherwise B  \n",
    "$\\hat{y}$ > 75%, classify as category A, otherwise B  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
